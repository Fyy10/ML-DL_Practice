# 第23周周报

## 学习内容

- 迁移学习 自适应问题

## 学习收获

### 基本概念和思想

对于一个实际的任务，可用的数据（带标签和不带标签的）通常是比较少的，数据难以收集和加标签，数据不足很难训练出好的模型

迁移学习目的在于用一个已经训练好的模型，通过对新的数据进行学习，使其能解决不同的任务，或者处理不同的数据

从人本身来想，原先学过的知识可以作为后面学新知识的基础，迁移学习于这一点是类似的

对于一个训练好的深度神经网络，前面几层所处理的信息是较为广泛的信息，比如图像的边界，特征等，后面几层处理的信息是较为特化的信息，这些信息大多对应于某种任务，比如分类。那么在实际应用中，可以把前面几层固定下来，因为已经可以正确提取图像的特征，仅对后面几层进行训练以完成不同的任务，这个过程在supervised learning中称为fine-tune

迁移学习所解决的就是从源域到目标域的转换

通常情况下，源域和目标域的feature map并不重合，甚至不相近，而adapt就是让两者feature map尽可能匹配，从而在源域上训练的模型也能拿到目标域上用

通常一个域由feature space $\mathcal{X}$ 和对应的概率分布 $P(X)$ 构成，$X = \{x_1, x_2, \dots, x_n\} \in \mathcal{X}$ ，用 $\mathcal{D} = \{\mathcal{X}, P(X)\}$ 来表示输入的feature space，$\mathcal{T}$ 表示所需要完成的任务，则迁移学习的过程如下：

$$
\mathcal{D}^s \Rightarrow \mathcal{D}^t\\
\mathcal{T}^s \Rightarrow \mathcal{T}^t
$$

其中

$$
\mathcal{T}^s = \{\mathcal{Y}^s, P(Y^s|X^s)\}\\
\mathcal{T}^t = \{\mathcal{Y}^t, P(Y^t|X^t)\}
$$

对于迁移学习，可以通过多个步骤来完成，而每一步，都可以把问题分成同构和异构的问题。

对于同构问题，输入的数据尽管不同，但是所在的空间相同，异构的连数据空间也不同。对于每一个问题，又可以根据数据是否由label分成supervised，semi-supervised和unsupervised learning，其中异构非监督学习是当前研究热点（也最复杂）

### 迁移方法

比较好理解的方法是固定已经训练好的模型的前面几层的参数，当作提取feature，再用新的数据训练后面几层进行fine-tune以完成其它任务，这是同构的supervised learning

对于同构的unsupervised learning，可以定义一个feature map上的差异，把这个差异加到loss上，缩小这个差异即完成了adaptation

差异经常使用Maximum Mean Discrepancy (MMD)：

$$
MMD^2(D_s, D_t) = \left\| {\frac{1}{M}\sum\limits_{i = 1}^M {\phi ({\rm{x}}_i^s) - } \frac{1}{N}\sum\limits_{j = 1}^N {\phi ({\rm{x}}_j^t)} } \right\|_H^2
$$

把这个MMD加到loss上，总的loss如下：

$$
\mathcal{L} = \mathcal{L}_C (X^L,y) + \lambda MMD^2 (X^s X^t)
$$

通过这个loss，可以使得在feature map上，源域和目标域中相似（同一类）的数据整合到一起，因此分类器可以像区分源域那样区分目标域

如果是异构的问题，可以用几层全连接层将问题映射到与源域相同的空间，再进行feature map上的分析和训练

也有基于AutoEncoder的方法，将提取特征的部分看作encoder，再建立一个decoder用于从目标域的特征生成源域的数据，当decoder成功做到这一点时，可认为从源域和目标域中提取出的特征是相似的

另外还有基于生成对抗网络的方法，用一个discriminator来区分feature map上的某个点是来自于源域还是目标域，discriminator要尽可能区分源域和目标域，discriminator不能区分时，可以认为源域和目标域提取出的特征被成功整合了（有点像是用NN来定义源域和目标域特征的差异）

上面写的一些方法都看起来比较简洁而且很有道理，但是实践的时候会有很多小问题，比如AE和GAN并不好训练，数据少的情况下fine-tune并不理想，选择哪一层作为feature比较玄学

还有一些在流形上分析的multi-step的方法，从目标域经过多步迁移到源域，由于以前没有接触过流形，内容也比较复杂，所以只算是有一点初步了解

## 疑问和困难

1. 从抽象意义上当黑盒处理比较好理解，但是有的理论涉及一些比较高深的概念（如流形），仅有的高数和线代概率论的知识感觉有些不够用，看得有些一头雾水
2. 尽管讲述的一些方法思路明确，但是代码貌似不太好写（比如把中间某层拿出来计算loss再组合到总loss中）
3. 异构的数据如何映射到同一个feature map，单纯地使用全连接层感觉不合适，因为一般情况下异构数据并不能跟源数据很好地对应
4. 如果把一个fine-tune过的神经网络用于原来的任务上，是否能获得好结果（个人认为是不能，需要如何改进使得两个任务都能胜任？）
5. 更为广泛一点的想法，异构而且任务也不相同，甚至是让一个网络完成多种任务的无监督问题要怎么处理（像管道那样训练提取中间部分，前端和后端的层根据特定任务另外设计和训练？）
